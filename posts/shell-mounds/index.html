<!doctype html><html lang=en><head><title>Detecting Shell Mounds in LiDAR Images</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.9f7114f80945d2cbd5fcb04087ae50b96aedd35ce9afe7c701c1caafc8ec79d0.css integrity="sha256-n3EU+AlF0svV/LBAh65QuWrt01zpr+fHAcHKr8jsedA="><link rel=icon type=image/png href=/images/site/favicon_hu3817522817642709778.png><meta property="og:url" content="https://via-zhang.github.io/posts/shell-mounds/"><meta property="og:site_name" content="Olivia Zhang"><meta property="og:title" content="Detecting Shell Mounds in LiDAR Images"><meta property="og:description" content="Reflecting on my semester conducting research at the AI for Bio/Cultural Diversity Lab."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-15T00:00:00+06:00"><meta property="article:modified_time" content="2024-04-15T00:00:00+06:00"><meta property="article:tag" content="Research"><meta property="article:tag" content="Data Analysis"><meta name=twitter:card content="summary"><meta name=twitter:title content="Detecting Shell Mounds in LiDAR Images"><meta name=twitter:description content="Reflecting on my semester conducting research at the AI for Bio/Cultural Diversity Lab."><meta name=description content="Reflecting on my semester conducting research at the AI for Bio/Cultural Diversity Lab."><script>theme=localStorage.getItem("theme-scheme")||localStorage.getItem("darkmode:color-scheme")||"light",theme=="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=80><div class="container-fluid bg-secondary wrapper"><nav class="navbar navbar-expand-xl top-navbar shadow" id=top-navbar><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<i data-feather=sidebar></i>
</button>
<a class=navbar-brand href=/><img src=/images/site/Logo_Green_hu698337351379312191.png id=logo alt=Logo>
Olivia Zhang</a>
<button class="navbar-toggler navbar-light" id=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#top-nav-items aria-label=menu>
<i data-feather=menu></i></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="nav navbar-nav ms-auto"><li class=nav-item><a class=nav-link href=/#home>Home</a></li><li class=nav-item><a class=nav-link href=/#about>About</a></li><li class=nav-item><a class=nav-link href=/#projects>Projects</a></li><li class=nav-item><a class=nav-link href=/#skills>Skills</a></li><li class=nav-item><a class=nav-link href=/#publications>Experiences</a></li><li class=nav-item><a class=nav-link href=/#education>Education</a></li><div id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/posts>Posts</a></li><li class=nav-item><a class=nav-link id=note-link href=/notes>Notes</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=theme-icon src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=theme-icon src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/Logo_Green_hu698337351379312191.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/Logo_White_hu15721562414338429140.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><a class=list-link href=/posts/trubel-summer/ title="Gulf Scholars Conference">Gulf Scholars Conference</a></li><li><a class=list-link href=/posts/gulf-scholars/ title="Gulf Scholars Conference">Gulf Scholars Conference</a></li><li><a class=list-link href=/posts/s3-summit/ title="Sustainability Summit">Sustainability Summit</a></li><li><a class="active list-link" href=/posts/shell-mounds/ title="Detecting Shell Mounds">Detecting Shell Mounds</a></li><li><a class=list-link href=/posts/miami-vibrance/ title="Vibrance of Miami">Vibrance of Miami</a></li><li><a class=list-link href=/posts/bcls-fellow/ title="Community Fellow">Community Fellow</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/shell-mounds/figure8.png)></div><div class=page-content><div style=margin-bottom:80px></div><div class=title><h1>Detecting Shell Mounds in LiDAR Images</h1></div><div class="author-profile ms-auto align-self-lg-center"><p class=text-muted>Monday, April 15, 2024 | 12 minutes</p></div><div class=tags><ul style=padding-left:0><li class=rounded><a href=/tags/research/ class="btn btn-sm btn-info">Research</a></li><li class=rounded><a href=/tags/data-analysis/ class="btn btn-sm btn-info">Data Analysis</a></li></ul></div><div class=post-content id=post-content><h2 id=project-overview>Project Overview</h2><p>Shell mounds, or shell middens, are hills of accumulated shell (clams, oysters, and whelks), mud bricks, pottery shards, and other objects built up by generations of Indigenous peoples (Orengo et al., 2020; Florida Natural Areas Inventory, 2010). Shell mounds, found along the coastlines of the southeastern United States (Florida Natural Areas Inventory, 2010), give us a look at the past, helping us understand the lives of communities from hundreds to thousands of years ago. As such, they are a crucial part of our cultural landscape—imprinted evidence of history and heritage on the natural landscape (Taylor & Lennon, 2011).</p><p>Shell mounds can be small or large—some acting as foundations on which new structures are built—with complex structures and ridges. The region studied through our research, the southwest coast of Florida, is the ancestral territory of the Calusa. The Calusa built complex shell mounds, middens, canals, and other earthworks (Charlotte County Historic Preservation Element, 2007), leaving a significant archeological imprint on the landscape. The accumulation of shell mounds provides archaeologists with crucial knowledge about past communities, from what types of food they ate to how large their communities were. Some sites, such as the shell midden on Josslyn Island, can protect from storm surges (Patterson, 2017, p. 34). However, many shell mounds have been destroyed or otherwise altered. For example, the Wightman Site on Sanibel Island was once a large shell midden, but has since been destroyed to make room for a housing development (Patterson, 2017, p. 48). This poses a significant question: How can we leverage emerging technologies to help preserve the shell mounds of Florida’s coast?</p><p>At the AI for Bio/Cultural Diversity Lab at the Florida Museum of Natural History, we aimed to answer this question by conducting AI-driven research about these significant archaeological sites. During the spring 2024 term, we tested different machine learning models to detect shell mounds in aerial images. With LiDAR elevation data and multi-scale topographic images of shell mounds across the west coast of Florida, we fine-tuned the following pre-existing models to detect shell mounds within images: Mask-RCNN, Grounding DINO, and YOLOv8.</p><p>Future work for the project involves 1) improving the performance of the object detection model with more training data and 2) using LiDAR and other remotely sensed data to expand our understanding of landscapes that have since been destroyed or sunken underwater.</p><h2 id=land-acknowledgement>Land Acknowledgement</h2><p>The University of Florida’s main campus is located on the ancestral territory of the Potano and of the Seminole peoples. The area of southwest Florida encompassed in our research is the ancestral territory of the Calusa. We acknowledge the long-lasting and devastating impact of colonization, and we honor and respect the Indigenous peoples who have, for thousands of years, lived and continue to live here.</p><h2 id=current-results>Current Results</h2><p>We began by analyzing what types of data can distinguish shell mounds from the surrounding landscape—starting with satellite images from Sentinel-2. Sentinel-2 images have twelve spectral bands, and different combinations of these bands indicate various qualities about the vegetation, soil, and more. For example, the Normalized Difference Vegetation Index (NDVI) is calculated with the near infrared and red bands as (NIR - R) / (NIR + R), which helps us visualize healthy or unhealthy vegetation. Using the <a href="https://CRAN.R-project.org/package=rsi" target=_blank rel=noopener>rsi package in R</a>, we retrieved Sentinel-2 images around prominent shell mound locations like Mound Key and Otter Mound, and we viewed <a href=https://github.com/awesome-spectral-indices/awesome-spectral-indices/blob/main/output/spectral-indices-table.csv target=_blank rel=noopener>over 150 spectral indices</a>. For the following vegetation indices, the shell mounds were especially prominent: Near-Infrared Reflectance of Vegetation (NIRv), Normalized Difference Vegetation Index (NDVI), Modified Chlorophyll Absorption in Reflectance Index (MCARI705), Red-Edge Disease Stress Index (REDSI), and Triangular Vegetation Index (TriVI).</p><img src=figure1.png width=50% class=center><p>To train our machine learning model, we used the locations of archeological sites for the west coast of Florida from <a href=https://dos.fl.gov/historical/preservation/master-site-file/ target=_blank rel=noopener>Florida Master Site File</a>. Specifically, we used data about shell mounds and shell middens from Lee County and Charlotte County.</p><p>First, we tested random forest classification using the programming language R. We used a 12-band Sentinel-2 image of Pine Island and calculated the vegetation indices NDVI and TriVI. We tested two methods: 1) using 1000 points labeled with all categories of archeological sites from the Florida Master Site File, such as prehistoric mound(s), shell midden, canal, and agriculture/farm structure, and 2) using 1000 points labeled as either “mound” or “not mound.”</p><img src=figure2.png width=90% class=center><p>After additional tests and finding that the results were not very accurate, we decided to 1) look more into object detection methods, and 2) focus on detecting shell mounds rather than multiple archaeological sites. Further, after reviewing previous research about detecting shell mounds and archaeological sites, we decided to use high-resolution elevation data rather than vegetation data—elevation is especially relevant because of the structure and height of shell mounds. Using the <a href=https://rvt-py.readthedocs.io/en/latest/index.html target=_blank rel=noopener>Relief Visualization Toolbox</a> (RVT), a set of tools to visualize elevation data, we processed LiDAR-derived digital elevation model (DEM) images to visualize local dominance and multi-scale topographic position.</p><p>The following figures show the Sentinel-2 image, LiDAR-derived digital elevation from the National Centers for Environmental Information (NCEI) and the United States Geological Survey (USGS), and the local dominance and multi-scale topographic position images made from the elevation data using RVT.</p><img src=figure3.png width=90% class=center><h3 id=object-detection-models>Object Detection Models</h3><p>We fine-tuned three open-source object detection models: Mask-RCNN, Grounding DINO, and YOLOv8. These models have already been trained to recognize objects in millions of everyday images. While these models most likely will not recognize shell mounds right away, they can be fine–tuned—a process for further training a model to recognize new objects while keeping their old information. I learned that fine-tuning is more efficient than training a model from the beginning, as the pre-existing models provide a foundation for further training. We tested and fine-tuned the models using the programming language Python and open-source code.</p><p>The first model was Mask-RCNN, implemented in the <a href=https://github.com/facebookresearch/detectron2 target=_blank rel=noopener>Detectron2</a> framework developed by Facebook AI Research, which draws boxes around different objects in an image and then segments (or traces out) the objects. We fine-tuned Detectron2 using <a href=https://rvt-py.readthedocs.io/en/latest/listofvis_localdom.html target=_blank rel=noopener>local dominance</a> images from the Relief Visualization Toolbox with LiDAR-derived elevation data from the National Centers for Environmental Information. In the local dominance images, lighter colors indicate areas that are higher than the surrounding land—making it helpful for identifying shell mounds. Once trained, the model was able to predict some of the shell mounds, but with low accuracy.</p><img src=figure4.png width=90% class=center><p>The second model we tested was <a href=https://github.com/IDEA-Research/GroundingDINO target=_blank rel=noopener>Grounding DINO</a>, which takes in a text prompt and identifies the prompt (which, in this case, is “shell mound” or a similar key phrase) within the image. The images shown below show using the pre-existing <a href=https://huggingface.co/spaces/merve/Grounding_DINO_demo target=_blank rel=noopener>Grounding DINO model</a> without any fine-tuning on local dominance and multi-scale topographic position images, using key phrases like “shell mound, “white mound,” “small bump” and “mound on flat map.” We are still in the process of finding a way to efficiently fine–tune Grounding DINO—this model requires the training data to be in a specific format, and the fine-tuning code is run in a slightly different environment than the other two models.</p><img src=figure5.png width=90% class=center><p>After testing these first two models, we decided to use <a href=https://rvt-py.readthedocs.io/en/latest/listofvis_mstp.html target=_blank rel=noopener>multi-scale topographic position</a> images from the Relief Visualization Toolbox instead of local dominance. The multi-scale topographic position, also derived from elevation data, provides more detailed information—it shows the relative topographic position of each pixel across different spatial scales: local, meso, and broad (Lindsay et al., 2015).</p><p>The third model we tested was <a href=https://github.com/ultralytics/ultralytics target=_blank rel=noopener>YOLOv8</a> (You Only Look Once, version 8), the first version of which was created in 2015 as an efficient object detection approach. One challenge to training the models was that we had a small training dataset of around 75 images (640 by 640 pixels each). To solve this, we flipped and rotated around a fourth of the images to get a final training set of 131 images.</p><p>Below shows the results from fine-tuning the model with 15 epochs on 131 images. The fourth graph (precision) indicates the percent of model-identified mounds that are real mounds, and the fifth graph (recall) indicates the percent of real mounds identified by the model. As the model is trained on the dataset through each epoch, the performance generally improves. Overall, these charts tell us that the model can accurately identify some shell mounds, but there is still a lot of room for improvement.</p><img src=figure6.png width=90% class=center><p>The model draws boxes around the predicted mounds and provides a number from 0 to 1 showing the confidence in the prediction (from 0 being not confident to 1 being the most confident). Below shows some images from the training dataset and the prediction results.</p><img src=figure7.png width=90% class=center><p>Below is the result from running the model on one large image, and many of the predictions are correct. We used the Python <a href=https://github.com/obss/sahi target=_blank rel=noopener>sahi library</a> to divide the image into 224 “slices,” and then ran the model on the smaller slices—helping the model detect small shell mounds. Without sahi, the model did not predict any mounds in this image—most likely because it was too zoomed out.</p><img src=figure8.png width=50% class=center><p>From exploring vegetation and elevation data to preparing 100+ training images, I learned a lot from testing the random forest classification and fine-tuning the three object detection models. There is future research to be done to improve the performance of the model with more training data while considering the performance metrics and results.</p><h3 id=side-quest-georeferencing-historical-aerial-images>Side Quest: Georeferencing Historical Aerial Images</h3><p>A side project we worked on this semester was georeferencing <a href=https://ufdc.ufl.edu/UF00071762/00002/citation target=_blank rel=noopener>Aerial Photographs of Lee County - Flight 2C (1944)</a> from the U.S. Department of Agriculture, recorded in the University of Florida’s Map and Imagery Library. The aerial images were very high resolution and provided a look at past landscapes. Using georeferencing tools in ArcGIS Pro, I imported the photographs, specifically those showing Pine Island, to resize and re-project them into a format easier to use with geographic information systems (GIS). I compared roads, lakes, and other features of the landscape to locate the photographs on the map.</p><p>Viewing these historical photographs gives us insights into how the landscape, including shell mounds, looked in the past. These images can be used to help decipher qualities about Florida’s landscape that has since been transformed. Once enough images were georeferenced around Pine Island, we could identify traces of an old, dried-up canal, which is no longer visible today—the canal is traced in cyan in the screenshot below.</p><img src=figure9.png width=50% class=center><h2 id=next-steps>Next Steps</h2><p>This research has various applications in archaeology and historic preservation—serving as a resource for us to understand the locations of archaeological sites and the imprints of past communities. As historic sites are increasingly threatened by sea level rise, extreme weather, and more issues exacerbated by climate change, historic preservation efforts are crucial to maintain and protect these sites.</p><p>Future work for this project includes finding more publicly-available high-resolution LiDAR data, further training the YOLOv8 model, and analyzing the model’s performance in detecting shell mounds. We can also further analyze how shell mounds and other archeological sites are impacted by climate change and extreme weather events like hurricanes. Using LiDAR data and machine learning methods, we can understand landscapes that have been destroyed or impacted by sea level rise.</p><p>As an undergraduate student at the University of Florida studying geography and data science, working on these projects has been an incredible learning experience—further motivating me to pursue a career in cultural geography and AI research. I learned how research and collaboration can contribute to historic preservation and help communities build resilience against climate change—leveraging data to better understand our cultural and natural landscapes.</p><h2 id=references>References</h2><div style=text-indent:-3em;padding-left:3em><p>Charlotte County. (2007). Chapter 9: Historic Preservation Element, 1997-2010 Comprehensive Plan Archive. Charlotte County, https://www.charlottecountyfl.gov/core/fileparse.php/376/urlt/chapter_9.pdf</p><p>Florida Natural Areas Inventory. (2010). Guide to the natural communities of Florida: 2010 edition. Florida Natural Areas Inventory, Tallahassee, FL. https://www.fnai.org/species-communities/natcom-guide</p><p>Orengo, H.A., Conesa, F.C., Garcia-Molsosa, A., Lobo, A., Green, A.S., Madella, M., Petrie, C.A. (2020). Automated detection of archaeological mounds using machine-learning classification of multisensor and multitemporal satellite data. Proceedings of the National Academy of Sciences, 117(31), 18240–18250, https://doi.org/10.1073/pnas.2005583117</p><p>Lindsay, J.B., Cockburn, J.M.H., Russell, H.A.J. (2015). An Integral Image Approach to Performing Multi-Scale Topographic Position Analysis. Geomorphology, 245, 51–61. https://doi.org/10.1016/j.geomorph.2015.05.025</p><p>Patterson, D. (2017). A tour of the islands of Pine Island Sound, Florida: Their geology, archaeology, and history (W. H. Marquardt, Ed.; Ser. 2). Randell Research Center, Florida Museum of Natural History.</p><p>Taylor, K., & Lennon, J. (2011). Cultural landscapes: a bridge between culture and nature? International Journal of Heritage Studies, 17(6), 537–554. https://doi.org/10.1080/13527258.2011.618246</p><h3 id=code-and-packages>Code and Packages</h3><p>Bhattiprolu, S. (2023). Train custom instance segmentation model using Detectron2 - on your own dataset. https://github.com/bnsreenu/python_for_microscopists/blob/master/330_Detectron2_Instance_3D_EM_Platelet.ipynb</p><p>Heras, J. Augmenting a dataset for object detection in YOLO. (n.d.). https://github.com/joheras/CLoDSA/blob/master/notebooks/CLODSA_YOLO.ipynb</p><p>Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al. (2023). Grounding dino: Marrying dino with grounded pre-training for open-set object detection. https://github.com/IDEA-Research/GroundingDINO</p><p>Mahoney, M. (2024). rsi: Efficiently Retrieve and Process Satellite Imagery. R package version 0.1.0, https://CRAN.R-project.org/package=rsi.</p><p>Pebesma, E. (n.d.). Random Forest land use classification, 7. Statistical modelling with stars objects. https://r-spatial.github.io/stars/articles/stars7.html#random-forest-land-use-classification</p><p>Piotr S. (2023). How to Train YOLOv8 Object Detection on a Custom Dataset. Roboflow Blog, https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/</p><p>R Core Team (2023). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org.</p><p>Siebring, J. (2023). GeoCOCO. https://github.com/jaspersiebring/GeoCOCO</p><p>Ultralytics. (n.d.). Model Training with Ultralytics YOLO. https://docs.ultralytics.com/modes/train/</p><p>Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. (2019). Detectron2. https://github.com/facebookresearch/detectron2</p><p>Žiga Kokalj, Žiga Maroh, Krištof Oštir, Klemen Zakšek and Nejc Čož. (2022). RVT Python library. https://github.com/EarthObservation/RVT_py</p><p>Zuwei Long, Wei Li. (2023). Open Grounding Dino: The third party implementation of the paper Grounding DINO. https://github.com/longzw1997/Open-GroundingDino</p><h3 id=data>Data</h3><p>2018 USGS Lidar DEM: Southwest, FL. NOAA Office for Coastal Management. https://noaa-nos-coastal-lidar-pds.s3.amazonaws.com/dem/USGS_FL_Southwest_2018_9049/index.html</p><p>Aerial Photographs of Lee County - Flight 2C (1944), U.S. Department of Agriculture, Map and Imagery Library, George A. Smathers Libraries, University of Florida. https://ufdc.ufl.edu/UF00071762/00002/images</p><p>Cooperative Institute for Research in Environmental Sciences (CIRES) at the University of Colorado, Boulder. (2014). Continuously Updated Digital Elevation Model (CUDEM) - 1/9 Arc-Second Resolution Bathymetric-Topographic Tiles. NOAA National Centers for Environmental Information. https://doi.org/10.25921/ds9v-ky35</p><p>Florida Master Site File. Florida Division of Historical Resources, Florida Department of State. https://dos.fl.gov/historical/preservation/master-site-file/</p><p>NOAA NCEI Continuously Updated Digital Elevation Model (CUDEM) - Ninth Arc-Second Resolution Bathymetric-Topographic Tiles, NOAA National Centers for Environmental Information (NCEI). https://chs.coast.noaa.gov/htdata/raster2/elevation/NCEI_ninth_Topobathy_2014_8483/FL/index.html</p></div></div><div class="row ps-3 pe-3"><div class="col-md-6 share-buttons"><strong>Share on:</strong>
<a class="btn icon-button bg-linkedin" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fvia-zhang.github.io%2fposts%2fshell-mounds%2f&title=Detecting%20Shell%20Mounds%20in%20LiDAR%20Images" target=_blank><i class="fab fa-linkedin"></i>
</a><a class="btn icon-button" href="mailto:?subject=Detecting%20Shell%20Mounds%20in%20LiDAR%20Images&body=https%3a%2f%2fvia-zhang.github.io%2fposts%2fshell-mounds%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/via-zhang/via-zhang.github.io/edit/main/content/posts/shell-mounds/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/s3-summit/ title="Geospatial Data Workshop at the Student Sustainability Summit" class="btn filled-button"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>Geospatial Data Workshop at the Student Sustainability Summit</div></a></div><div class="col-md-6 next-article"><a href=/posts/miami-vibrance/ title="Vibrance of Miami, a Data Storytelling Project" class="btn filled-button"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Vibrance of Miami, a Data Storytelling Project</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn type=button data-bs-toggle=tooltip data-bs-placement=left title="Scroll to top"><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center ps-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#project-overview>Project Overview</a></li><li><a href=#land-acknowledgement>Land Acknowledgement</a></li><li><a href=#current-results>Current Results</a><ul><li><a href=#object-detection-models>Object Detection Models</a></li><li><a href=#side-quest-georeferencing-historical-aerial-images>Side Quest: Georeferencing Historical Aerial Images</a></li></ul></li><li><a href=#next-steps>Next Steps</a></li><li><a href=#references>References</a><ul><li><a href=#code-and-packages>Code and Packages</a></li><li><a href=#data>Data</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-start"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://via-zhang.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://via-zhang.github.io/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://via-zhang.github.io/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://via-zhang.github.io/#publications>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://via-zhang.github.io/#education>Education</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=https://github.com/via-zhang target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>via-zhang</span></a></li><li><a href=https://www.linkedin.com/in/via-zhang target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>Olivia Zhang</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-start"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu16779671404603505019.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">2025 Portfolio</div><div class="col-md-4 text-end"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.2ca0d01bacaf8959e8347a7b4bd9f1f415918c030390666914debd42a1f585d3.js integrity="sha256-LKDQG6yviVnoNHp7S9nx9BWRjAMDkGZpFN69QqH1hdM=" defer></script></body></html>